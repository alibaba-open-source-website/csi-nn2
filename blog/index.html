<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Blog | SHL</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://csi-nn2.opensource.alibaba.com/blog"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | SHL"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://csi-nn2.opensource.alibaba.com/blog"><link data-rh="true" rel="alternate" href="https://csi-nn2.opensource.alibaba.com/blog" hreflang="en"><link data-rh="true" rel="alternate" href="https://csi-nn2.opensource.alibaba.com/zh/blog" hreflang="zh"><link data-rh="true" rel="alternate" href="https://csi-nn2.opensource.alibaba.com/blog" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="SHL RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="SHL Atom Feed">



<meta name="aes-config" content="pid=xux-opensource&amp;user_type=101&amp;uid=&amp;username=&amp;dim10=csi-nn2"><link rel="stylesheet" href="/assets/css/styles.53758859.css">
<link rel="preload" href="/assets/js/runtime~main.f0d05c9b.js" as="script">
<link rel="preload" href="/assets/js/main.b286ab1d.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script>
<script src="//g.alicdn.com/alilog/mlog/aplus_v2.js" id="beacon-aplus" exparams="clog=o&amp;aplus&amp;sidx=aplusSidx&amp;ckx=aplusCkx"></script>
<script src="//g.alicdn.com/aes/??tracker/1.0.34/index.js,tracker-plugin-pv/2.4.5/index.js,tracker-plugin-event/1.2.5/index.js,tracker-plugin-jserror/1.0.13/index.js,tracker-plugin-api/1.1.14/index.js,tracker-plugin-perf/1.1.8/index.js,tracker-plugin-eventTiming/1.0.4/index.js"></script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">SHL</b></a><a href="https://zhangwm-pt.github.io/shl/modules.html" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">API<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/blog" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh/blog" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh">简体中文</a></li></ul></div><a href="https://github.com/T-head-Semi/csi-nn2" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/RVM example">RISC-V matrix extension example</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/deploy on th1520">How to Deploy a Neural Network on TH1520</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/C908 accelerates AI">XuanTie C908 Accelerates AI with Software and Hardware Fusion</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/MLPerf Tiny">XuanTie C906 Tops MLPerf Tiny v0.7 Benchmark</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/RVM example">RISC-V matrix extension example</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-02-01T00:00:00.000Z" itemprop="datePublished">February 1, 2023</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/shaowg" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/shaowg.png" alt="Wengan Shao"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/shaowg" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Wengan Shao</span></a></div><small class="avatar__subtitle" itemprop="description">Engineer @ T-Head</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>In order to enhance the AI inference ability of the Xuantie processor, T-Head propose a matrix extension Instruction set. The following is an example of AI inference introduced through the T-Head open source AI deployment package.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-docker">1 docker<a class="hash-link" href="#1-docker" title="Direct link to heading">​</a></h2><p>Pull the image hhb:2.1-matrix that supports matrix extension from <a href="https://hub.docker.com/" target="_blank" rel="noopener noreferrer">docker hub</a>, start a container, and open a terminal in interactive mode in the container:</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> pull hhb4tools/hhb:2.1-matrix</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> run -itd --name</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">your.hhb2.1-matrix -p </span><span class="token number" style="color:#36acaa">22</span><span class="token plain"> -v /your_mount_dir/:/mnt </span><span class="token string" style="color:#e3116c">&quot;hhb4tools/hhb:2.1-matrix&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">docker</span><span class="token plain"> </span><span class="token builtin class-name">exec</span><span class="token plain"> -it your.hhb2.1-matrix /bin/bash</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>After entering container, you can use the hhb --version command to confirm:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">root@c14249f8243c:/</span><span class="token comment" style="color:#999988;font-style:italic"># hhb --version</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">HHB version: </span><span class="token number" style="color:#36acaa">2.1</span><span class="token plain">.x-matrix, build </span><span class="token number" style="color:#36acaa">20230131</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>docker installation guide：<a href="https://docs.docker.com/engine/install/" target="_blank" rel="noopener noreferrer">Docker Engine installation overview</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-model-deployment">2 Model deployment<a class="hash-link" href="#2-model-deployment" title="Direct link to heading">​</a></h2><p>Take the deployment of <a href="https://github.com/shicai/MobileNet-Caffe" target="_blank" rel="noopener noreferrer">MobileNet</a> as an example, in /home/rvm_caffe_mv1_int8, there is already a complete Makefile script, execute the  <code>make</code>  command to convert the model into the required sample program, which can be executed on RISC-V architecture chips that support matrix extension.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token builtin class-name">cd</span><span class="token plain"> /home/rvm_caffe_mv1_int8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">make</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The key steps in the model deployment process are described below:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-model-compilation">2.1 Model compilation<a class="hash-link" href="#21-model-compilation" title="Direct link to heading">​</a></h3><p>HHB is an offline AI model compilation and optimization tool. Executing the following commands can quantize the original model, optimize operators such as operator fusion, and generate a C code model with high execution efficiency on the target chip.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hhb -C --calibrate-dataset ./cat.jpg --model-file ./mobilenetv1.prototxt ./mobilenetv1.caffemodel --data-scale </span><span class="token number" style="color:#36acaa">0.017</span><span class="token plain"> --data-mean </span><span class="token string" style="color:#e3116c">&#x27;104 117 124&#x27;</span><span class="token plain"> --output </span><span class="token builtin class-name">.</span><span class="token plain"> --board rvm --quantization-scheme</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;int8_asym_w_sym&quot;</span><span class="token plain"> --pixel-format BGR --fuse-conv-relu --channel-quantization --target-layout NHWC</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The model compilation options are described as:</p><ul><li>-C ：specifies to execute the main command until C code is generated.</li><li>--calibrate-dataset：specifies the calibration image used for quantization.</li><li>--model-file ：specifies a MobileNet model downloaded to the current directory. A Caffe model is divided into two files. The files following the option are not sequence-sensitive.</li><li>--data-mean ：specifies a mean.</li><li>--data-scale ：specifies a scale.</li><li>--output ：specifies the current directory as the path to store files that you need to generate.</li><li>--board ：specifies the platform as the destination platform.</li><li>--quantization-scheme ：specifies a quantization scheme. </li><li>--pixel-format：specifies the input image format required by the model, the default is RGB, and the BGR image needs to be set to BGR when the model is trained.</li><li>--fuse-conv-relu：specifies relu fuse to convolution layer.</li><li>-channel-quantization：specifies weight channel quantization.</li><li>--target-layout NHWC：specifies the tensor layout.</li></ul><p>After the command is executed, multiple files such as main.c and model.c will be generated in the current directory:</p><ul><li>data.0.tensor： cat.png preprocessed tensor by decoding.</li><li>data.0.bin：data.0.tensor binary data.</li><li>main.c：the reference entry to the sample program.</li><li>model.c：a model structure file that describes the model structure.</li><li>hhb.bm：HHB format model file.</li><li>model.params：the weights converted to int8.</li><li>io.c：the helper function for reading and writing files.</li><li>io.h：the declaration of the helper function for reading and writing files.</li><li>process.c：the image preprocessing function.</li><li>process.h：the declaration of the image preprocessing function.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-shl-library">2.2 SHL library<a class="hash-link" href="#22-shl-library" title="Direct link to heading">​</a></h3><p>SHL is a set of neural network library API for Xuantie CPU platform, and provides a series of optimized binary libraries. SHL supports convolutional layer-focused optimization by matrix extension, . In this example, the prebuilt inference library has been placed in the /home/install_nn2 directory, and the source code can also be downloaded and rebuild by the following steps.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">git</span><span class="token plain"> clone -b matrix https://github.com/T-head-Semi/csi-nn2.git</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token builtin class-name">cd</span><span class="token plain"> csi-nn2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">make</span><span class="token plain"> nn2_rvm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">make</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">install</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-executable-program">2.2 Executable program<a class="hash-link" href="#22-executable-program" title="Direct link to heading">​</a></h3><p>After hhb completes the code generation, execute the following compilation command to link the rvm high-performance library and generate the c_runtime program in the current directory:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">riscv64-unknown-linux-gnu-gcc -O2 -g3 -march</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">rv64gcv_zfh_xtheadc -mabi</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">lp64d -I/home -I/home/install_nn2/include -I/home/decode/install/include -o c_runtime  main.c model.c io.c process.c -L/home/install_nn2/lib -L/home/decode/install/lib/rv -ljpeg -lpng -lz -lstdc++ -lshl_rvm -lm -static -Wl,--gc-sections</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The compilation option is described as follows:</p><ul><li>-O2 -g3: specifies the optimization option and debug-level.</li><li>-march: specifies the architecture option for RISC-V matrix extension chip.</li><li>-mabi: specifies the application binary interface (ABI) option for RISC-V matrix extension chip.</li><li>-I: specifies the location of the header file that needs to be used during compilation.</li><li>main.c model.c io.c process.c: the source file that you need to use for compilation.</li><li>-L: specifies the path to store the specified library.</li><li>-ljpeg: links to a JPEG decoding library.</li><li>-lpng: links to a PNG decoding library.</li><li>-lz: links to a zlib.</li><li>-lstdc++: links to a standard C++ library.</li><li>-lshl_rvm: links to an optimized version library of rvm in SHL.</li><li>-lm: links to a standard math library.</li><li>-static: a static link.</li><li>-Wl,--gc-sections: recycles unused sections during linking.</li></ul><p>The gcc version used in this example is V2.6.1, you can use the following command to check:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">riscv64-unknown-linux-gnu-gcc -v</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-simulate">3 Simulate<a class="hash-link" href="#3-simulate" title="Direct link to heading">​</a></h2><p>After the compilation is complete, use T-Head&#x27;s qemu simulation program to execute, and you can see the top5 execution results on the terminal:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qemu-riscv64 -cpu  rv64,x-v</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">true,vext_spec</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">v1.0,vlen</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">128</span><span class="token plain">,x-matrix</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">on,mlen</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">128</span><span class="token plain"> c_runtime model.params data.0.bin</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><img loading="lazy" src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/330274/1675153172385-5bf1da34-1ba2-4549-9620-f266e733660c.png#clientId=ua527e72f-b31b-4&amp;from=paste&amp;height=407&amp;id=KD9lI&amp;name=image.png&amp;originHeight=488&amp;originWidth=1291&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;size=215314&amp;status=done&amp;style=none&amp;taskId=u50db042f-2f84-4107-9d4f-6a137e9b30c&amp;title=&amp;width=1075.8332905835593" alt="image.png" class="img_ev3q">
The qemu version used in this example is V6.0.94, you can use the following command to check:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">qemu-riscv64 -version</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-other">4 Other<a class="hash-link" href="#4-other" title="Direct link to heading">​</a></h2><p>RISC-V matrix extension also supports fp16 data type, just modify the hhb compilation command as follows, and keep other steps unchanged, you can use fp16 for inference.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hhb -C --calibrate-dataset ./cat.jpg --model-file ./mobilenetv1.prototxt ./mobilenetv1.caffemodel </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--data-scale </span><span class="token number" style="color:#36acaa">0.017</span><span class="token plain"> --data-mean </span><span class="token string" style="color:#e3116c">&#x27;104 117 124&#x27;</span><span class="token plain"> --output </span><span class="token builtin class-name">.</span><span class="token plain"> --board rvm --quantization-scheme</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;float16&quot;</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">--pixel-format BGR --target-layout NHWC</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/matrix-extension">matrix extension</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/example">example</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/deploy on th1520">How to Deploy a Neural Network on TH1520</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-01-11T00:00:00.000Z" itemprop="datePublished">January 11, 2023</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/zhangwm-pt" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/zhangwm-pt.png" alt="Wenmeng Zhang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zhangwm-pt" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Wenmeng Zhang</span></a></div><small class="avatar__subtitle" itemprop="description">Engineer @ T-Head</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><p>T-Head has recently introduced a high-performance SoC prototyping, i.e. TH1520, which is built on the Wujian600 chip development platform. With a quad-core XuanTie C910 CPU withbuilt-in 4-TOPS NPU, TH1520 engenders a new combination of CPU and AI computing. </p><p>In this blog, we will describe the process of how to deploy a neural network model on C910 and on C910 and NPU simultaneously.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tools">Tools<a class="hash-link" href="#tools" title="Direct link to heading">​</a></h2><p>T-Head offers two open-source deployment tools that enable seamless, highly efficient integration of NN frameworks and underlying hardware:</p><ul><li>Heterogeneous Honey Badge)(HHB): It supports models from different NN frameworks, and provides quantization and graph optimization.</li><li>Structure of Heterogeneous Library (SHL): It is a common interface that is compatible with all hardware types, whil offering a reference schedule that facilitates software portability.</li></ul><p><img loading="lazy" alt="Tool flow" src="/assets/images/ali135-b3fcec4e8dbd3972a20aafcc21b3cde0.png" width="880" height="524" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hhb">HHB<a class="hash-link" href="#hhb" title="Direct link to heading">​</a></h3><p><a href="https://www.xrvm.com/tool-details?id=4056748601592913921#Download" target="_blank" rel="noopener noreferrer">HHB</a> is a collection of tools provided by T-Head to deploy neural network models on XuanTie processors. These tools can be incorporated for compilation, profiling, and simulation. </p><p>Its framework is based on Apache TVM, which is an end-to-end machine learning compiler structure. We have shared the source code on <a href="https://github.com/T-head-Semi/tvm" target="_blank" rel="noopener noreferrer">GitHub</a>.</p><p>HHB supports models such as Caffe, TensorFlow, ONNX, and TensorFlow Lite. It can convert these models into unified intermediate expressions for graphing performance optimization.</p><p>In addition, HHB supports multiple quantization methods to handle various data types. This framework can automatically provide the optimal scheme for the specified XuanTie CPU platform. After quantization, HHB generates a graph structure in C code from the intermediate expression. Each node of the graph structure is constructed by calling the CSI-NN2 API.</p><p>Here is an example to use HHB in deploying MobileNet model on TH1520. The sample code shows the hhb command to compile the model:</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hhb -C --board light --calibrate-dataset ./cat.jpg --model-file ./mobilenetv1.prototxt ./mobilenetv1.caffemodel --data-mean </span><span class="token string" style="color:#e3116c">&quot;103.94 116.98 123.68&quot;</span><span class="token plain"> --data-scale </span><span class="token number" style="color:#36acaa">0.007843</span><span class="token plain"> --output </span><span class="token builtin class-name">.</span><span class="token plain"> --quantization-scheme</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&quot;int8_asym&quot;</span><span class="token plain"> --pixel-format BGR</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The following content describes the parameter options:</p><ul><li>C: specifies to execute the main command until C code is generated.</li><li>board: emphasizes as the destination platform; light is an alias of TH1520.</li><li>calibrate-dataset: specifies the calibration image used for quantization.</li><li>model-file: specifies a MobileNet model downloaded to the current directory. A Caffe model is divided into two files. The files following the option are not sequence-sensitive.</li><li>data-mean: defines a mean.</li><li>data-scale: defines a scale.</li><li>output: describes the current directory as the path to store files that you need to generate.</li><li>quantization-scheme: identifies a quantization scheme. </li><li>pixel-format: identifies the input image format required by the model training.</li></ul><p>After the command is executed, multiple files such as main.c and model.c are generated in the current directory:</p><ul><li>main.c: the reference entry to the sample program.</li><li>model.c: a model structure file that describes the model.</li><li>hhb.hm: the weights converted to int8.</li><li>io.c: the helper function for reading and writing files.</li><li>io.h: the declaration of the helper function for above files.</li><li>process.c: the image preprocessing function.</li><li>process.h: the declaration of the above function.</li></ul><p>After the HHB command generates code, the gcc command performs binary encoding.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">riscv64-unknown-linux-gnu-gcc -O0 -g3 -march</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">rv64gcv0p7_zfh_xtheadc -mabi</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">lp64d -I/home -I/home/install_nn2/include -I/home/decode/install/include -o c_runtime  main.c model.c io.c process.c -L/home/install_nn2/lib -L/home/decode/install/lib/rv -ljpeg -lpng -lz -lstdc++ -lshl_rvv -lm -static -Wl,--gc-sections</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The following content describes the parameter options:</p><ul><li>O0 -g3: specifies the optimization option. In this example, you can use the debug-level O0 only.</li><li>march: identifies the architecture option for C910.</li><li>mabi: identifies the application binary interface (ABI) option for C910.</li><li>I: describes the location of the header file that is used during compilation.</li><li>o: describes the name of the executable file needed to generate.</li><li>main.c model.c io.c process.c: the source file yu for compilation.</li><li>L: specifies the path to store the specified library.</li><li>ljpeg: links to a JPEG decoding library.</li><li>lpng: links to a PNG decoding library.</li><li>lz: links to a zlib.</li><li>lstdc++: links to a standard C++ library.</li><li>lshl_rvv: links to an optimized version library of C910 in SHL.</li><li>lm: links to a standard math library.</li><li>static: a static link.</li><li>Wl,–gc-sections: recycles unused sections during linking.</li></ul><p>After the compilation is complete, the c_runtime file is created under the current directory. Copy the hhb.bm file and the cat.jpg image that are generated by incorporating the hhb command and the c_runtime file to the development board of C910 to execute at a time:</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">./c_runtime hhb.bm cat.jpg</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>You can view the top 5 execution results on the terminal. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="shl">SHL<a class="hash-link" href="#shl" title="Direct link to heading">​</a></h3><p>SHL, previously called CSI-NN2,  is a neural network acceleration library.</p><p>It abstracts various common neural network operators to form unified interfaces. SHL also implements an acceleration library for XuanTie CPU. This interface offers optimization code at the assembly level for the RISC-V Vector extension. The acceleration library has adapted to multiple data types of quantization schemes.</p><p>Combined with the automatic quantization function of HHB, SHL can quickly change the original model from the single-precision floating-point data type to optimal. As a result,the model can deliver the best performance on the development board.</p><p>The source code of SHL has been made available on <a href="https://github.com/T-head-Semi/csi-nn2" target="_blank" rel="noopener noreferrer">GitHub</a>.</p><p>SHL shares the specifications of RISCV-V Vector extension V0.7.1 in the implementation of the neural network operator on XuanTie C910. Considering the features of the CPU hardware (such as pipeline dependence, branch prediction, or cache), SHL fully excavates the parallel capabilities of the fp16 data format in the algorithm.</p><p>To balance performance and accuracy, some SoCs may have an NPU to accelerate some int8 neural network operators. SHL provides one reference schedule module to find the best processor for  operators.</p><p><img loading="lazy" alt="graph opt" src="/assets/images/ali136-73a2f322deb96fd8df1252539baab274.png" width="1202" height="610" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="c910-performance">C910 Performance<a class="hash-link" href="#c910-performance" title="Direct link to heading">​</a></h2><p>XuanTie C910 is a 64-bit high-performance processor based on the 64-bit RISC-V architecture. This processor adopts a state-of-the-art 12-stage and out-of-order multiple issue superscalar pipeline. On TH1520, it can clock up to 2.5GHz. It is also equipped with 128-bit vector operation units to deliver optimized performance. </p><p>The vector operation units of XuanTie C910 are designed following version 0.7.1 of RISC-V Vector Extension. C910 supports wide-ranging data formats, including int8, int16, int32, int64, bf16, fp16, fp32, and fp64. fp16 is the default format for deploying network models, with which Xuantie C910 can achieve its best performance.</p><p>We have tested various typical image classification models. The table below presents the performance of our deployment software on C910 at 1.85 GHz.</p><p><img loading="lazy" alt="c910 perf" src="/assets/images/ali138-2ba85047243dc356868ada350815adcd.png" width="1238" height="406" class="img_ev3q"></p><p>As a comparison, <a href="https://github.com/google/XNNPACK" target="_blank" rel="noopener noreferrer">XNNPACK</a> costs 77ms (multi-threaded) to infer a MobileNet model on Raspberry Pi 4B.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="c910-and-npu">C910 and NPU<a class="hash-link" href="#c910-and-npu" title="Direct link to heading">​</a></h2><p>In order to accelerate the convolution operator in the neural network, TH1520 is equipped with a 4-TOPS NPU. The NPU can also expedite more than 20 other operators in the neural network under int8.</p><p>The table below presents the performance of combining C910 and NPU to access typical image classification models:</p><p><img loading="lazy" alt="npu perf" src="/assets/images/ali139-ee83d8b897492361bbc070ea55945ac9.png" width="1178" height="320" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>This article describes in details on how to deploy a neural network model on TH1520, We have also presented optimal performance of TH1520 in basic image classification tasks.</p><p>TH1520 has already been incorporated inside Alibaba’s ecosystem, which demonstrates the feasibility of RISC-V-based high-performance devices to deploy neural network models. In addition, the source code of deployment tools, HHB and SHL, has been open-sourced and shared on GitHub.</p><p><a href="https://www.xrvm.com/" target="_blank" rel="noopener noreferrer">Read more details related to the hardware and software.</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/th-1520">TH1520</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/c-910">C910</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/npu">NPU</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/wujian-600">Wujian600</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/C908 accelerates AI">XuanTie C908 Accelerates AI with Software and Hardware Fusion</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-12-20T00:00:00.000Z" itemprop="datePublished">December 20, 2022</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/shaowg" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/shaowg.png" alt="Wengan Shao"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/shaowg" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Wengan Shao</span></a></div><small class="avatar__subtitle" itemprop="description">Engineer @ T-Head</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-introduction">1. Introduction<a class="hash-link" href="#1-introduction" title="Direct link to heading">​</a></h2><p><a href="https://xrvm.com/?spm=a2cl5.27298783.0.0.6618272dxfNBdQ" target="_blank" rel="noopener noreferrer">XuanTie C908</a> is the latest RISC-V processor released by T-Head Semiconductor, It has a frequency of up to 2 GHz. which allows it to be widely used in visual AI, intelligent interaction, and other advanced technologies. This article focuses on an array of topics from processor micro-architecture, to convolution acceleration algorithm, to optimized operators for XuanTie C908. We are also showcasing the AI inference performance of XuanTie C908 by using the T-Head open source AI deployment kit for the first time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-ai-acceleration-of-hardware-and-software-integration">2. AI acceleration of hardware and software integration<a class="hash-link" href="#2-ai-acceleration-of-hardware-and-software-integration" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-processor-micro-architecture">2.1 Processor micro-architecture<a class="hash-link" href="#21-processor-micro-architecture" title="Direct link to heading">​</a></h3><ul><li>Support instruction fusion technology</li><li>Compliant with RISC-V vector extension 1.0</li><li>Support 128/256 configurable vector register bit width VLEN</li><li>The vector execution unit supports FP16/BFP16/FP32 floating point and INT8/INT32/INT64 integer operations</li><li>Support INT8/INT4 vector dot product operations.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-software-algorithm-optimization">2.2 Software algorithm optimization<a class="hash-link" href="#22-software-algorithm-optimization" title="Direct link to heading">​</a></h3><p>Structure of Heterogeneous Library (SHL) is a set of neural network library APIs for XuanTie CPU platform. It abstracts common neural network operator interfaces. For the newly released XuanTie C908, SHL provides the inference acceleration of multiple data types (fp32/fp16/int8). Combined with the processor pipeline, instruction fusion, and high-speed cache technology, it offers deep assembly optimization for core operators in neural networks.</p><p>Convolution has been the most crucial operator in CNN models. Currently, im2col + GEMM and Winograd are supported in SHL to accelerate convolution calculations. The main steps of Winograd are:</p><ul><li>Input padding</li><li>Input transformation</li><li>Input reordering</li><li>Batch GEMM operations</li><li>Output transformation</li><li>Output cropping</li></ul><p>The core computing of the two algorithms is gemm. The following figure uses vlen128/fp16 as an example to show the calculation process of gemm.</p><p><img loading="lazy" alt="im1" src="/assets/images/image1-a6712ce9864380f64ee02799f82445d4.png" width="1544" height="753" class="img_ev3q"></p><p>Vector load (vle) is used for weight data, while scalar load (flh) for input data. This design takes 16*12 register blocks to improve computational efficiency by performing outer product matrix. We manually remove read-after-write and write-after-write data dependencies to adjust instruction flow. Last but not least, we have incorporated advanced instruction fusion technology to fully optimize performance of XuanTie C908. (The arrows in the figure indicate the arrangement order of the data in the memory.)</p><p>The list of optimized operators supported by SHL for XuanTie C908 is as follows:</p><ul><li>conv2d</li><li>depthwiseconv2d</li><li>maxpool2d</li><li>avgpool2d</li><li>global_maxpool2d</li><li>global_avgpool2d</li><li>fullyconnected</li><li>relu</li><li>relu6</li><li>leaky_relu</li><li>prelu</li><li>sigmoid</li><li>softmax</li><li>concat</li><li>pad</li><li>elementwise_add</li><li>elementwise_mul</li><li>sum</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-model-deployment">2.3 Model deployment<a class="hash-link" href="#23-model-deployment" title="Direct link to heading">​</a></h2><p>Heterogenous Honey Badger (HHB) has been adapted to the latest XuanTie C908 processor. It supports weight symmetric, activation asymmetric int8 data type quantization and fp16 data type quantization. One only needs a simple command to generate the C code model file for inference on XuanTie C908. While calling on the SHL XuanTie C908 high-performance inference computing library, you can achieve the best performance experience of model inference on XuanTie C908.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">hhb -C –calibrate-dataset ./cat.jpg –model-file ./mobilenetv1.prototxt ./mobilenetv1.caffemodel –data-scale </span><span class="token number" style="color:#36acaa">0.017</span><span class="token plain"> –data-mean ‘104 </span><span class="token number" style="color:#36acaa">117</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">124</span><span class="token plain">’ –output </span><span class="token builtin class-name">.</span><span class="token plain"> –board c908 –quantization-scheme</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">”int8_asym_w_sym” –pixel-format BGR –fuse-conv-relu –channel-quantization</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-performance">3. Performance<a class="hash-link" href="#3-performance" title="Direct link to heading">​</a></h2><p>We tested the AI inference performance of some common CNN models on XuanTie C908 using HHB and SHL. After adding the int8 vector dot product instruction, we improved XuanTie C908 performance by 3.35 times on mobilenet. This step enables us to expand the vector length to 256 results in a speedup ratio of 1.55 to 1.68. The AI performance provided by XuanTie C908 (@vlen128) has been increased by 3.75 to 4.57 times compared with that of the previous generation XuanTie C906 (@D1).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-vector-dot-product-extension">3.1 Vector dot product extension<a class="hash-link" href="#31-vector-dot-product-extension" title="Direct link to heading">​</a></h3><p><img loading="lazy" alt="im2" src="/assets/images/image2-2889b0a2867be573e6c92bde2652cef2.png" width="633" height="421" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-vlen256-and-vlen128">3.2 Vlen256 and vlen128<a class="hash-link" href="#32-vlen256-and-vlen128" title="Direct link to heading">​</a></h3><p><img loading="lazy" alt="im3" src="/assets/images/image3-688daf53a1f0e7ece62a219e7d6361c4.png" width="606" height="408" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-xuantie-c908-and-xuantie-c906">3.3 XuanTie C908 and XuanTie C906<a class="hash-link" href="#33-xuantie-c908-and-xuantie-c906" title="Direct link to heading">​</a></h3><p><img loading="lazy" alt="im4" src="/assets/images/image4-cbb81c159bd691048bd25a39b6894c18.png" width="600" height="360" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-conclusion">4. Conclusion<a class="hash-link" href="#4-conclusion" title="Direct link to heading">​</a></h2><p>XuanTie C908 greatly improves AI computing power and performance. We have followed the standard RISC-V vector extension 1.0 and supported int8/int4 vector dot product extensions. Thus, we have provided 256-bit wide vector register configurable options for Xuantie C908. This article describes the specific steps of integration with micro-architecture and instruction characteristics of the XuanTie C908 processor. By doing so, we are able to accelerate the convolution operator in CNN and introduces the SHL high-performance computing library GEMM optimization ideas and the list of optimized operators. Moreover, we have compared the AI performance of XuanTie C908 and the previous generation XuanTie C906, further highlighting the potential and advantages of the RISIC-V processor architecture in the field of AIOT through software and hardware joint optimization.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/c-908">C908</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hardware">Hardware</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/MLPerf Tiny">XuanTie C906 Tops MLPerf Tiny v0.7 Benchmark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-06-13T00:00:00.000Z" itemprop="datePublished">June 13, 2022</time> · <!-- -->5 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/zhangwm-pt" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/zhangwm-pt.png" alt="Wenmeng Zhang"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/zhangwm-pt" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Wenmeng Zhang</span></a></div><small class="avatar__subtitle" itemprop="description">Engineer @ T-Head</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>XuanTie C906 is a processor developed by Alibaba Cloud based on the RISC-V instruction set architecture. It has attained top marks in the most recent findings from <a href="https://mlcommons.org/en/inference-tiny-07/" target="_blank" rel="noopener noreferrer">MLPerf Tiny v0.7</a>, an AI benchmark focusing on IoT devices. The performance of XuanTie C906 excelled in all four core categories: Visual Wake Words (VWW), Image Classifications (IC), Keyword Spotting (KWS), and Anomaly Detection (AD).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-mlperf-tiny">About MLPerf Tiny<a class="hash-link" href="#about-mlperf-tiny" title="Direct link to heading">​</a></h2><p>MLPerf Tiny Inference is a benchmark developed by MLCommons. It is designed to measure the efficiency of processing new data by a trained neural network for extremely low-power devices., as well as providing an optional power measurement test.</p><p>The benchmark consists of four machine learning tasks that involve using microphone and camera sensors within embedded devices<sup>1</sup>:</p><ul><li>Keyword Spotting (KWS):  a feature that utilizes a neural network to detect keywords from a spectrogram</li><li>Visual Wake Words (VWW): a binary image classification task to determine the presence of a person in an image</li><li>Tiny Image Classification (IC):  a small image classification benchmark with 10 classes</li><li>Anomaly Detection (AD): uses a neural network to identify abnormalities in machine operating sounds</li></ul><p>The image below details the results.</p><p><img loading="lazy" alt="res" src="/assets/images/ali1-1-60d4957d7a6009cd19cead98223a2b62.png" width="1876" height="940" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="xuantie-c906-with-the-risc-v-vector-extension">XuanTie C906 with the RISC-V Vector Extension<a class="hash-link" href="#xuantie-c906-with-the-risc-v-vector-extension" title="Direct link to heading">​</a></h2><p>XuanTie C906 is a 64-bit high-energy processor based on a 64-bit RISC-V architecture. This processor is designed with a five to eight stage integer pipeline. It is also equipped with 128-bit vector operation units to deliver excellent performance. Not only does XuanTie C906 adopt a multi-channel and mode data prefetching technologies, it improves and optimizes data access bandwidth and prefetching.</p><p>The vector operation units of XuanTie C906 are designed to follow the specifications of RISC-V Vector extension V0.7.1. Data formats, including int8, int16, int32, int64, bf16, fp16, fp32, and fp64, are supported. In the benchmark we have used f16 as the default, with which Xuantie C906 achieved the best performance.</p><p>The XuanTie C906 silicon chip is used in Allwinner SoC D1, which has been put into full-scale production. Allwinner D1 has been embedded in various development boards and is available in the open market.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="software-stack">Software Stack<a class="hash-link" href="#software-stack" title="Direct link to heading">​</a></h2><p><img loading="lazy" alt="stack" src="/assets/images/ali2-82a17c007815bae1e66cdb99ced708c2.png" width="1966" height="330" class="img_ev3q"></p><p>As shown in the preceding flowchart, the original model is obtained from MLPerf Tiny. An optional next step is to then be compressed by Sinian. Subsequently, Heterogeneous Honey Badger (HHB) converts the model to function library calls which are supported by the CSI-NN2 API. CSI-NN2 finally implements neural network interfaces by using the vector operation units of XuanTie C906.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="csi-nn2">CSI-NN2<a class="hash-link" href="#csi-nn2" title="Direct link to heading">​</a></h3><p>CSI-NN2 is a set of API interfaces for neural network acceleration libraries. It abstracts various common neural network operators to form unified interfaces.</p><p>CSI-NN2 also implements an acceleration library for XuanTie CPU. This interface provides optimization code at the assembly level for the RISC-V Vector extension. The acceleration library has adapted to multiple data types of quantization schemes.</p><p>Combined with the automatic quantization function of HHB, CSI-NN2 can quickly change the original model from the single-precision floating-point data type to optimal so that the model can deliver the best performance on the development board.</p><p>The source code of CSI-NN2 has been made available on <a href="https://github.com/T-head-Semi/csi-nn2" target="_blank" rel="noopener noreferrer">GitHub</a>.</p><p>CSI-NN2 shares the specifications of RISCV-V Vector extension V0.7.1 in the implementation of neural network operator on XuanTie C906. Considering the features of the CPU hardware (such as pipeline dependence, branch prediction, or cache), CSI-NN2 fully excavates the parallel capabilities of the fp16 data format in the algorithm.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hhb">HHB<a class="hash-link" href="#hhb" title="Direct link to heading">​</a></h3><p>HHB is a collection of tools provided by T-Head to deploy neural network models on XuanTie processors. These tools can be incorporated for compilation, profiling, and simulation. The framework is based on Apache TVM, which is an end-to-end machine learning compiler structure. </p><p>The source code of HHB has been shared on <a href="https://github.com/T-head-Semi/tvm" target="_blank" rel="noopener noreferrer">GitHub</a>.</p><p>HHB supports the network model formats of Caffe, TensorFlow, ONNX, and TensorFlow Lite. It can convert these model formats into unified intermediate expressions for graphing performance optimization. </p><p>In addition, HHB supports multiple quantization methods to handle various data types. This framework can automatically provide the optimal  scheme for the specified XuanTie CPU platform. After quantization, HHB generates a graph structure in C code from the intermediate expression. Each node of the graph structure is constructed by calling the CSI-NN2 API.</p><p>As a common deployment tool set, HHB can also access the original model in the benchmark with the following performance:</p><p><img loading="lazy" alt="perf" src="/assets/images/ali3-c71c23fa46827bd23820923b687b9601.png" width="808" height="298" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="sinian">Sinian<a class="hash-link" href="#sinian" title="Direct link to heading">​</a></h3><p>Sinian is a computing acceleration platform for neural network models. It utilizes technologies for model compression such as network structure search and knowledge distillation.</p><p>In the benchmark, Sinian has reduced the calculation workload of every model by three to eight times.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>This article describes the results the XuanTie C906 attained in the MLPerf Tiny v0.7 benchmark in terms of performance. XuanTie C906 implements the specifications of RISC-V Vector extension V0.7.1. It has been put into scale production and is available on <a href="https://sipeed.aliexpress.com/store/group/RISC-V/1101739727_10000003584258.html?spm=a2g0o.store_pc_home.smartGrouping_6001928813303.10000003584258" target="_blank" rel="noopener noreferrer">AliExpress</a>. In addition, the source code of CSI-NN2 and HHB have been open sourced and shared on GitHub.</p><p><a href="https://occ.t-head.cn/" target="_blank" rel="noopener noreferrer">Read more details related to the hardware and software.</a></p><p>References:
<!-- -->[1]<!-- --> MLPerf Tiny Inference Benchmark. from: <a href="https://mlcommons.org/en/news/mlperf-tiny-v05/" target="_blank" rel="noopener noreferrer">https://mlcommons.org/en/news/mlperf-tiny-v05/</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/d-1">D1</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/c-906">C906</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.yuque.com/za4k4z/kkzsw9/cs7ms4" target="_blank" rel="noopener noreferrer" class="footer__link-item">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://xrvm.com/" target="_blank" rel="noopener noreferrer" class="footer__link-item">T-HEAD Open Chip Community<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/T-head-Semi/csi-nn2" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 T-Head, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.f0d05c9b.js"></script>
<script src="/assets/js/main.b286ab1d.js"></script>
</body>
</html>