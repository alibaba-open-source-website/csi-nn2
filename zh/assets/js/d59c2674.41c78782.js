"use strict";(self.webpackChunkcsi_nn_2=self.webpackChunkcsi_nn_2||[]).push([[7484],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var u=r.createContext({}),l=function(e){var t=r.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=l(e.components);return r.createElement(u.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,u=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=l(n),m=i,h=d["".concat(u,".").concat(m)]||d[m]||p[m]||a;return n?r.createElement(h,o(o({ref:t},c),{},{components:n})):r.createElement(h,o({ref:t},c))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,o=new Array(a);o[0]=d;var s={};for(var u in t)hasOwnProperty.call(t,u)&&(s[u]=t[u]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var l=2;l<a;l++)o[l]=n[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8135:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=n(7462),i=(n(7294),n(3905));const a={sidebar_position:3},o="API usage",s={unversionedId:"tutorial-basics/api_usage",id:"tutorial-basics/api_usage",title:"API usage",description:"Frame Structure",source:"@site/docs/tutorial-basics/api_usage.md",sourceDirName:"tutorial-basics",slug:"/tutorial-basics/api_usage",permalink:"/zh/docs/tutorial-basics/api_usage",draft:!1,editUrl:"https://github.com/alibaba-open-source-website/easyexcel/tree/main/docs/tutorial-basics/api_usage.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Build from source",permalink:"/zh/docs/tutorial-basics/build_source"},next:{title:"Tutorial - Extras",permalink:"/zh/docs/category/tutorial---extras"}},u={},l=[{value:"Frame Structure",id:"frame-structure",level:2},{value:"Layer execution",id:"layer-execution",level:2},{value:"Sample code",id:"sample-code",level:3},{value:"Graph Execution",id:"graph-execution",level:2},{value:"Sample code",id:"sample-code-1",level:3}],c={toc:l};function p(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"api-usage"},"API usage"),(0,i.kt)("h2",{id:"frame-structure"},"Frame Structure"),(0,i.kt)("p",null,"SHL can be divided into the following modules: "),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Per-layer API\uff1aThe operator of the neural network model is used as a unit to provide separate docking interfaces for common operators. In addition, auxiliary interfaces are provided to assist in managing the execution of the entire network model."),(0,i.kt)("li",{parentName:"ol"},"Reference Runtime\uff1a the Reference implementation of heterogeneous execution engines, including subgraph division and memory allocation. "),(0,i.kt)("li",{parentName:"ol"},"Vector Instruction OPT\uff1athe Instruction set customized for the xuantie series CPU. "),(0,i.kt)("li",{parentName:"ol"},"Driver Wrapper\uff1athe implementation of various NPU Driver interfaces. ")),(0,i.kt)("p",null,"Per-layer API connects to different deployment tool frameworks based on different target platforms: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"directly call the Vector Instruction OPT to perform layer-by-layer execution. "),(0,i.kt)("li",{parentName:"ul"},"directly call the Driver Wrapper structure diagram to send the complete diagram to the NPU Driver for execution. "),(0,i.kt)("li",{parentName:"ul"},"call the Reference Runtime and pass in the sub-graph partitioning instructions. Reference Runtime choose to schedule different acceleration hardware to execute different sub-graphs. ")),(0,i.kt)("p",null,"According to the major categories of execution, SHL interfaces can be used in two main ways: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"layer execution"),(0,i.kt)("li",{parentName:"ul"},"graph Execution ")),(0,i.kt)("h2",{id:"layer-execution"},"Layer execution"),(0,i.kt)("p",null,"for computing units that do not require driver assistance, such as CPU and DSP, we recommend that you use interfaces by executing each layer in sequence."),(0,i.kt)("p",null,"Take a model with only one convolution layer and one relu layer as an example: "),(0,i.kt)("h3",{id:"sample-code"},"Sample code"),(0,i.kt)("p",null,"the simplest layer-by-layer code is as follows. After the csinn_conv2d function is executed, the convolution execution result can be obtained from output0. after the csinn_relu function is executed, the relu execution result can be obtained from output1."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-c"},"struct csinn_tensor *input = input_init();\nstruct csinn_tensor *output0 = output0_init();\nstruct csinn_tensor *kernel = kernel_init();\nstruct csinn_tensor *bias = bias_init();\nstruct csinn_conv2d_params *params0 = params0_init();\n\ncsinn_conv2d(input, output0, kernel, bias, params0);\n\nstruct csinn_tensor *output1 = output1_init();\nstruct csinn_relu_params *params1 = params1_init();\n\ncsinn_relu(output0, output1, params1);\n\n")),(0,i.kt)("p",null,"The initialization values of csinn_tensor, csinn_relu_params, and csinn_conv2d_params. For more information, see chapter data structure description"),(0,i.kt)("h2",{id:"graph-execution"},"Graph Execution"),(0,i.kt)("p",null,"for computing units that require large-scale driver assistance, such as NPU and GPU, they often have their custom graph structures. We recommend that you use the SHL layer to construct a graph. Finally, the graph is executed in accordance with the graph.\nWhen heterogeneous computing is required on the target platform, we recommend that you use graph execution."),(0,i.kt)("h3",{id:"sample-code-1"},"Sample code"),(0,i.kt)("p",null,"compared with layer-by-layer execution, an additional structure csinn_session is introduced to store the graph structure.\nThe sample code is as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-c"},"csinn_() {\n    /* alloc session */\n    csinn_session_init(session);\n    csinn_set_input_number(1, session);\n    csinn_set_output_number(1, session);\n\n    /* init input, output0, kernel, bias, params0 */\n    ...\n\n    /* set input as graph's first node */\n    csinn_pnna_input_setup(input, session);\n\n    /* graph_input_index = 0 */\n    csinn_set_input(graph_input_index, input, session);\n\n    csinn_conv2d(input, output0, kernel, bias, &params0);\n\n    /* init output1, params1 */\n    ...\n\n    csinn_relu(output0, output1, params1);\n\n    /* graph_output_index = 0 */\n    csinn_set_output(graph_output_index, output1, session);\n\n    /* call compiler */\n    csinn_session_setup(session);\n}\n\ncsinn_run() {\n    csinn_update_input(graph_input_index, input_tensor, session);\n    csinn_session_run(session);\n}\n\n")),(0,i.kt)("p",null,"The preceding code is divided into two functions: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"construction diagram "),(0,i.kt)("li",{parentName:"ul"},"execution chart")),(0,i.kt)("p",null,"The process of constructing the graph is as follows: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"initialize the session and set the number of inputs and outputs. "),(0,i.kt)("li",{parentName:"ul"},"initialize the input node of the graph. Generally, the input node of the graph is also the first node of the entire graph. "),(0,i.kt)("li",{parentName:"ul"},"csinn_set_input specifies the input node in the figure and the number of inputs the node is. "),(0,i.kt)("li",{parentName:"ul"},"call csinn_conv2d, csinn_relu, and add operator nodes to the graph. "),(0,i.kt)("li",{parentName:"ul"},"csinn_set_output is the node of the output specified in the figure and the number of outputs the node is. "),(0,i.kt)("li",{parentName:"ul"},"csinn_session_setup is the end of graph construction. Generally, the dynamic compiler in the driver is called here to compile the graph into NPU executable binary. ")),(0,i.kt)("p",null,"The execution process is divided into two steps: "),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"set the input data required to execute the model"),(0,i.kt)("li",{parentName:"ul"},"execution chart")))}p.isMDXComponent=!0}}]);